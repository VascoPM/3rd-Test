{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "799e2b5a-775a-49d6-98af-53d20db4394f",
   "metadata": {},
   "source": [
    "# Fully Connected Auto-Encoder wb Sweeps\n",
    "### $Time$ $Series$ $3rd$ $Test$\n",
    "\n",
    "$Vasco$ $Mergulh√£o$ $-$ $Jan$ $2023$\n",
    "\n",
    "### Version 1:\n",
    " - Applies Weights and Biases Sweeps on a given Sub-Sample.\n",
    " - Only uses FC-AE architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e09955-8b34-471f-9666-6be4c3d7d84b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ANN Configurations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7006642b-d7cf-492d-912c-7b3ba615f4df",
   "metadata": {
    "tags": []
   },
   "source": [
    "- #### Architecture(s)\n",
    "    - Fully Connected Auto Encoder\n",
    "        - Small (Input, 200, 200, Enconder)\n",
    "        - Medium (Input, 300, 300, 300, 300, Encoder)\n",
    "        - End2End [other papers orig ref] (Input, 500, 500, 2000, Encoder)\n",
    "    \n",
    "- #### Hyperparamenters\n",
    "    - Latent Space Size\n",
    "    - Batch Size\n",
    "        - Small test [2 - 32] and Large test [128 - 256]\n",
    "    - Learning Rate\n",
    "    - Learning Rate Scheduler\n",
    "        - Performance Schedulling\n",
    "    - Activation Functions\n",
    "        - SELU and Leaky ReLU\n",
    "    - Initializations\n",
    "        - LeCun and He (accordingly)\n",
    "    - Batch Normalization\n",
    "        - With/Without tests (note: if data is not z-scored, SELU not worth it, downgrade to ELU)\n",
    "    - Optimizers\n",
    "        - Nadam and SDG(momentum [0.9], Nesterov)\n",
    "    - Epochs\n",
    "        - 100 with Early Stopping\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1205a3-bfeb-40f8-9106-b3598695acb6",
   "metadata": {},
   "source": [
    "---\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50a0cab-db27-40d4-be64-2087fe36b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta, date\n",
    "import time\n",
    "import datetime\n",
    "import scipy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, LeakyReLU, Activation\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow import keras\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c19390-b3cd-49c5-82dc-b0618be84be2",
   "metadata": {},
   "source": [
    "---\n",
    "# Script Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e07d23-de4a-4f84-8ce5-fcad4e8db1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines Dataset for the Sweep\n",
    "dataset_name = 'Kenya_10k_Set_1_w90'\n",
    "\n",
    "# Uses name to navigate folders\n",
    "dataset_folder = \"_\".join(dataset_name.split('_')[:-1]) #Takes out window length section\n",
    "dataset_location = f'Data/{dataset_folder}/{dataset_name}.csv'\n",
    "\n",
    "# Zcore Data Decision\n",
    "zscore_data = True # Set to: [True/False]\n",
    "zscore_data_done = False # Always set to False. Ensures its not normalized multiple times\n",
    "\n",
    "# Model Name and Variables\n",
    "AE_Model_Name = 'FC_Small' # Options: FC_Small, FC_Medium\n",
    "latent_layer_size = 5\n",
    "\n",
    "# Sweep Names and Configurations\n",
    "Project_Name = f'FC_AE-{dataset_name}'\n",
    "Sweep_Config = f'{AE_Model_Name}_v1'\n",
    "sweep_count = 1\n",
    "use_running_sweep_id = True # Set to: [True/False]\n",
    "running_sweep_id = 'ikwjt20r'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe15b696-cd03-4bd7-a797-f8e276c060ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_col_names(dataset_name, win_prefix = 'd'):\n",
    "    # retriving window length\n",
    "    window_len = int(dataset_name.split('_')[-1][1:]) # Gets _wXX part of name, then skips 'w' to get the number.\n",
    "    # defining window column names\n",
    "    window_cols = [None]*window_len\n",
    "    for i  in range(window_len):\n",
    "        window_cols[i] = f'{win_prefix}' + str(i+1)\n",
    "        \n",
    "    return window_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e7f7a-21f0-4ec6-9f62-679309343302",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa08f090-20ce-4dad-a0fd-d45bc11be782",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454b1a56-50a5-4de5-910a-21d1a1a02a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_ID</th>\n",
       "      <th>window_ID</th>\n",
       "      <th>window_start_date</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>...</th>\n",
       "      <th>d81</th>\n",
       "      <th>d82</th>\n",
       "      <th>d83</th>\n",
       "      <th>d84</th>\n",
       "      <th>d85</th>\n",
       "      <th>d86</th>\n",
       "      <th>d87</th>\n",
       "      <th>d88</th>\n",
       "      <th>d89</th>\n",
       "      <th>d90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>347</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>15.422222</td>\n",
       "      <td>14.422222</td>\n",
       "      <td>13.422222</td>\n",
       "      <td>12.422222</td>\n",
       "      <td>11.422222</td>\n",
       "      <td>10.422222</td>\n",
       "      <td>9.422222</td>\n",
       "      <td>...</td>\n",
       "      <td>8.696088</td>\n",
       "      <td>6.709653</td>\n",
       "      <td>6.696088</td>\n",
       "      <td>5.696088</td>\n",
       "      <td>3.730243</td>\n",
       "      <td>3.696088</td>\n",
       "      <td>1.740706</td>\n",
       "      <td>0.744016</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-04-19</td>\n",
       "      <td>6.475880</td>\n",
       "      <td>4.490544</td>\n",
       "      <td>4.475880</td>\n",
       "      <td>3.475880</td>\n",
       "      <td>2.475880</td>\n",
       "      <td>1.475880</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>6.634468</td>\n",
       "      <td>5.634468</td>\n",
       "      <td>4.634468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>347</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>3.634468</td>\n",
       "      <td>2.634468</td>\n",
       "      <td>1.634468</td>\n",
       "      <td>3.634468</td>\n",
       "      <td>2.634468</td>\n",
       "      <td>1.634468</td>\n",
       "      <td>0.634468</td>\n",
       "      <td>...</td>\n",
       "      <td>4.762303</td>\n",
       "      <td>3.762303</td>\n",
       "      <td>2.762303</td>\n",
       "      <td>1.762303</td>\n",
       "      <td>0.762303</td>\n",
       "      <td>5.762303</td>\n",
       "      <td>4.762303</td>\n",
       "      <td>3.762303</td>\n",
       "      <td>2.762303</td>\n",
       "      <td>1.762303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-10-16</td>\n",
       "      <td>0.762303</td>\n",
       "      <td>1.762303</td>\n",
       "      <td>0.762303</td>\n",
       "      <td>2.762303</td>\n",
       "      <td>1.762303</td>\n",
       "      <td>1.762303</td>\n",
       "      <td>0.762303</td>\n",
       "      <td>...</td>\n",
       "      <td>3.553345</td>\n",
       "      <td>2.553345</td>\n",
       "      <td>1.553345</td>\n",
       "      <td>0.553345</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>6.646019</td>\n",
       "      <td>5.646019</td>\n",
       "      <td>4.646019</td>\n",
       "      <td>3.646019</td>\n",
       "      <td>2.646019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>347</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>1.646019</td>\n",
       "      <td>0.646019</td>\n",
       "      <td>2.646019</td>\n",
       "      <td>1.646019</td>\n",
       "      <td>0.646019</td>\n",
       "      <td>2.646019</td>\n",
       "      <td>1.646019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613681</td>\n",
       "      <td>2.613681</td>\n",
       "      <td>1.613681</td>\n",
       "      <td>1.613681</td>\n",
       "      <td>0.613681</td>\n",
       "      <td>1.613681</td>\n",
       "      <td>2.613681</td>\n",
       "      <td>1.613681</td>\n",
       "      <td>1.613681</td>\n",
       "      <td>0.613681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   short_ID  window_ID window_start_date         d1         d2         d3  \\\n",
       "0       347          0        2018-01-19  15.422222  14.422222  13.422222   \n",
       "1       347          1        2018-04-19   6.475880   4.490544   4.475880   \n",
       "2       347          2        2018-07-18   3.634468   2.634468   1.634468   \n",
       "3       347          3        2018-10-16   0.762303   1.762303   0.762303   \n",
       "4       347          4        2019-01-14   1.646019   0.646019   2.646019   \n",
       "\n",
       "          d4         d5         d6        d7  ...       d81       d82  \\\n",
       "0  12.422222  11.422222  10.422222  9.422222  ...  8.696088  6.709653   \n",
       "1   3.475880   2.475880   1.475880 -7.000000  ... -7.000000 -7.000000   \n",
       "2   3.634468   2.634468   1.634468  0.634468  ...  4.762303  3.762303   \n",
       "3   2.762303   1.762303   1.762303  0.762303  ...  3.553345  2.553345   \n",
       "4   1.646019   0.646019   2.646019  1.646019  ...  0.613681  2.613681   \n",
       "\n",
       "        d83       d84       d85       d86       d87       d88       d89  \\\n",
       "0  6.696088  5.696088  3.730243  3.696088  1.740706  0.744016 -7.000000   \n",
       "1 -7.000000 -7.000000 -7.000000 -7.000000 -7.000000  6.634468  5.634468   \n",
       "2  2.762303  1.762303  0.762303  5.762303  4.762303  3.762303  2.762303   \n",
       "3  1.553345  0.553345 -7.000000  6.646019  5.646019  4.646019  3.646019   \n",
       "4  1.613681  1.613681  0.613681  1.613681  2.613681  1.613681  1.613681   \n",
       "\n",
       "        d90  \n",
       "0 -7.000000  \n",
       "1  4.634468  \n",
       "2  1.762303  \n",
       "3  2.646019  \n",
       "4  0.613681  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820e9e9-9ffd-4180-be09-cb39525f933b",
   "metadata": {},
   "source": [
    "---\n",
    "# Z-Scoring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f716844-a03b-4118-a9bf-dd3605c75bed",
   "metadata": {},
   "source": [
    "This is done on a row-by-row basis.<br>\n",
    "Meaning, each window is normalized to its own Mean and Std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec3cd0f-131b-4936-8093-3560828065c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Zscore (df_in, all_neg_replace = -1):\n",
    "    # retriving sets of columns\n",
    "    window_cols = window_col_names(dataset_name)        \n",
    "    cols = df_in.columns.to_list()\n",
    "    other_cols = list(set(cols) - set(window_cols))\n",
    "    \n",
    "    # Zscore can't handle constant (flat) inputs\n",
    "    # Therefore windows with all negative credit output NaNs\n",
    "    # Instead these are excluded from zcoring and are manually scaled\n",
    "    # Default scalling is  -7 => -1\n",
    "    all_neg_replace = float(all_neg_replace)\n",
    "    all_neg_index = df_in[(df_in[window_cols] == float(-7)).all(axis=1)].index\n",
    "    positive_index = list(set(df_in.index.to_list()) - set(all_neg_index))\n",
    "    \n",
    "    if len(all_neg_index.to_list()) > 0:\n",
    "        # Aux DF for replacing -7 with all_neg_replace (i.e, -1)\n",
    "        df_allneg = df_in.iloc[all_neg_index].copy()\n",
    "        df_allneg.loc[:, window_cols] = all_neg_replace\n",
    "\n",
    "        # Zscoring only non-constant rows\n",
    "        df_positive = df_in.iloc[positive_index].copy()    \n",
    "        df_positive[window_cols] = scipy.stats.zscore(df_positive[window_cols],\n",
    "                                                         axis=1,\n",
    "                                                         nan_policy = 'omit')\n",
    "\n",
    "        df_zscore = pd.concat([df_positive, df_allneg])\n",
    "        df_zscore.sort_values(by=['short_ID', 'window_ID'], inplace = True)\n",
    "\n",
    "        # If indeces match, then the join was successful\n",
    "        if df_zscore.index.to_list() == df_in.index.to_list():\n",
    "            return df_zscore\n",
    "        # Otherwise raise error\n",
    "        else:\n",
    "            print('Error Zscoring')\n",
    "    else:\n",
    "        df_zscore = pd.DataFrame(columns= cols)\n",
    "        df_zscore[window_cols] = scipy.stats.zscore(df_in[window_cols],\n",
    "                                                         axis=1,\n",
    "                                                         nan_policy = 'omit')\n",
    "        df_zscore[other_cols] = df_in[other_cols]\n",
    "        \n",
    "\n",
    "    return df_zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26abf739-b1f7-47cb-8a4e-c0a8e3a091e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data WAS Zscored\n"
     ]
    }
   ],
   "source": [
    "if zscore_data == True and zscore_data_done == False:\n",
    "    Data = Zscore(Data)\n",
    "    zscore_data_done = True\n",
    "    print('Data WAS Zscored')\n",
    "    \n",
    "elif zscore_data == True and zscore_data_done == True:\n",
    "    print('Already WAS Zscored')\n",
    "    \n",
    "elif zscore_data == False:\n",
    "    print('Data NOT Zscored')\n",
    "    \n",
    "else:\n",
    "    print('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4715eb-fd23-4b2b-a649-0dbb93dac42b",
   "metadata": {},
   "source": [
    "---\n",
    "# NaN Checks and Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "604ef535-f512-482f-b7ac-d9a5618293d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaN_policy (df, method_fullrows = 'ffill', method_sparserows = 'ffill'):\n",
    "    # This functions checks and corrects NaNs.\n",
    "    # It has redundancy built into it to double check for NaNs.\n",
    "    # And allows for different policies for sparse NaN values and rows full of NaNs.\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "    window_cols = window_col_names(dataset_name)\n",
    "        \n",
    "    if df[window_cols].isna().any().any():\n",
    "        print('NaN values detected.')\n",
    "        \n",
    "        nan_exist = True\n",
    "        i = 0   \n",
    "        while nan_exist:\n",
    "            \n",
    "            # Makes sure the policy only iterates with one redundant round.\n",
    "            if i >= 2:\n",
    "                print('NaN Policy Failed')\n",
    "                break            \n",
    "\n",
    "            #Checking if there are rows with only NaNs\n",
    "            if df[window_cols].isna().all(axis=1).any():\n",
    "                n_nan_rows = len(df[df[window_cols].isna().all(axis=1)])\n",
    "                print(f'There are {n_nan_rows} rows of all NaN values.')\n",
    "\n",
    "                #Correcting only NaN rows \n",
    "                df.fillna(method= method_fullrows, axis = 0, inplace = True)\n",
    "                if ~ df[window_cols].isna().all(axis=1).any():\n",
    "                    print(f'Rows of NaN corrected')\n",
    "                    if ~ df[window_cols].isna().any().any():\n",
    "                        nan_exist = False\n",
    "                        print(f'NaNs no longer detected.')\n",
    "            \n",
    "            #Checking if there are rows with any NaNs in them\n",
    "            if df[window_cols].isna().any(axis=1).any():\n",
    "                n_nan_rows = len(df[window_cols].isna().any(axis=1))\n",
    "                print(f'There are {n_nan_rows} rows with some NaN values.')\n",
    "                \n",
    "                # Correcting NaN inside rows \n",
    "                df.fillna(method= method_sparserows, axis = 1, inplace = True)\n",
    "                if ~ df[window_cols].isna().any(axis=1).any():\n",
    "                    print(f'Rows with some NaN were corrected')\n",
    "                    if ~ df[window_cols].isna().any().any():\n",
    "                        nan_exist = False\n",
    "                        print(f'NaNs no longer detected.')\n",
    "\n",
    "            i += 1\n",
    "            \n",
    "\n",
    "    else:\n",
    "        print('No NaNs detected')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cab94776-f23c-4f78-828e-9d9f95856d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values detected.\n",
      "There are 22 rows of all NaN values.\n",
      "Rows of NaN corrected\n",
      "NaNs no longer detected.\n"
     ]
    }
   ],
   "source": [
    "Data = NaN_policy(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f32c5ba-c5cd-429f-83b1-04802f37c7aa",
   "metadata": {},
   "source": [
    "---\n",
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d11e337e-ea76-4f36-8103-84be3dda4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_cols = window_col_names(dataset_name)\n",
    "data_array = Data[window_cols].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe373fa-c589-484a-979f-7bb8f0a2674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(data_array, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72bc06f-26b7-4bc1-9200-b5a7b65a8e1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# WandB Sweep Config\n",
    "https://github.com/wandb/examples/blob/master/colabs/keras/Keras_param_opti_using_sweeps.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2e3e110-cecc-4b2e-ad62-a72a0932aa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvasco-mergulhao\u001b[0m (\u001b[33mvasco-phd\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba57bc94-c803-41bc-a1a8-c8b3be884590",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Model & Sweep Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36d8021-e521-447d-addb-21d8d35727b0",
   "metadata": {},
   "source": [
    "## Generic Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f342333-502d-4d47-b4c9-dec9e4901246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initialization (activation_fn = 'SELU'):\n",
    "    # Select right initialization for respective activation function between SELU and LeakyReLU\n",
    "    if activation_fn.lower() == \"selu\":\n",
    "        return 'lecun_normal'\n",
    "    if activation_fn.lower() == \"leakyrelu\":\n",
    "        return 'he_normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfdd6e5c-b329-4a02-9d10-72f568141656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn (activation_fn = 'SELU'):\n",
    "    # Select right initialization for respective activation function between SELU and LeakyReLU\n",
    "    if activation_fn.lower() == \"selu\":\n",
    "        return Activation('selu')\n",
    "    if activation_fn.lower() == \"leakyrelu\":\n",
    "        return LeakyReLU(alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "025ad319-1962-4e46-91a9-daa10e338c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(lr=0.0001, optimizer=\"nadam\"):\n",
    "    # Select optmizer between adam and sgd\n",
    "    if optimizer.lower() == \"nadam\":\n",
    "        return tf.keras.optimizers.Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    if optimizer.lower() == \"sgd\":\n",
    "        return tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "596c3d61-c6f1-42a8-847f-a81cc5e73e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, batch_size= 32, epochs= 100, lr=0.001, optimizer='nadam'):  \n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    model.compile(loss=\"mse\", \n",
    "                  optimizer=get_optimizer(lr, optimizer), \n",
    "                  metrics=[\"mse\"])\n",
    "\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    lr_scheduler_cb = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "    model.fit(x_train, \n",
    "              x_train, \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              validation_data=(x_test, x_test), \n",
    "              callbacks=[WandbCallback(), early_stopping_cb, lr_scheduler_cb])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77e4713a-6b32-4bc1-8afd-4f93c9bd73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_train(config_defaults=None):\n",
    "    # Initialize wandb with a sample project name\n",
    "    with wandb.init(config=config_defaults):  # this gets over-written in the Sweep\n",
    "\n",
    "        # Specify the other hyperparameters to the configuration\n",
    "        wandb.config.architecture_name = AE_Model_Name\n",
    "        wandb.config.dataset_name = dataset_name\n",
    "\n",
    "        # initialize model\n",
    "        if AE_Model_Name == 'FC_Small':\n",
    "            AE_model = Small_AE(wandb.config.window_length,\n",
    "                            wandb.config.latent_layer_size,\n",
    "                            wandb.config.activation_fn)\n",
    "        if AE_Model_Name == 'FC_Medium':\n",
    "            AE_model = Medium_AE(wandb.config.window_length,\n",
    "                            wandb.config.latent_layer_size,\n",
    "                            wandb.config.activation_fn)\n",
    "\n",
    "        train(AE_model, \n",
    "              wandb.config.batch_size, \n",
    "              wandb.config.epochs,\n",
    "              wandb.config.learning_rate,\n",
    "              wandb.config.optimizer)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34188b9-3983-439e-b1ea-8929a5f0b6b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FC_Small\n",
    "- (Input, 200, 200, Enconder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63ead0-4dc6-4265-aeaf-f6983bb85f77",
   "metadata": {},
   "source": [
    "### Sweep Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce7cfdf2-4a03-44a5-9640-018ee011075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Used\n"
     ]
    }
   ],
   "source": [
    "if AE_Model_Name == 'FC_Small':\n",
    "    \n",
    "    sweep_config = {\n",
    "    'method': 'random',\n",
    "    'name': Sweep_Config,\n",
    "    }\n",
    "    \n",
    "    metric = {\n",
    "    'name': 'mse',\n",
    "    'goal': 'minimize'\n",
    "    }\n",
    "    sweep_config['metric'] = metric\n",
    "    \n",
    "    parameters_dict = {\n",
    "        'optimizer': {\n",
    "            'values': ['nadam', 'sgd']\n",
    "        },\n",
    "        'latent_layer_size': {\n",
    "            'value': latent_layer_size #previous [values: [5, 10, 25]\n",
    "        },\n",
    "        'epochs':{\n",
    "            'value': 100\n",
    "        },\n",
    "        'window_length':{\n",
    "            'value': int(dataset_name.split('_')[-1][1:])\n",
    "        },\n",
    "        'activation_fn':{\n",
    "            'values': ['SELU','LeakyReLU']\n",
    "        }\n",
    "    }\n",
    "    sweep_config['parameters'] = parameters_dict\n",
    "    \n",
    "    parameters_dict.update({\n",
    "        'learning_rate': {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 0.001,\n",
    "            'max': 0.1,\n",
    "          },\n",
    "        'batch_size': {\n",
    "            # integers between 2 and 256\n",
    "            # with evenly-distributed logarithms \n",
    "            'distribution': 'q_log_uniform_values',\n",
    "            'q': 2,\n",
    "            'min': 2,\n",
    "            'max': 256,\n",
    "          }\n",
    "        })\n",
    "    print('Code Used')\n",
    "else:\n",
    "    print('NOT Active')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449e26d-42a9-4b10-86d0-2a2fb63ee390",
   "metadata": {},
   "source": [
    "### _Model_: FC_Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cc65c45-3dc7-405a-a6e0-d6128c122369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Small_AE(window_length = 90, latent_layer_size = 5, activation_fn = 'SELU'):\n",
    "    \n",
    "    inputs = Input(shape= window_length)\n",
    "    \n",
    "    layer_e1 = Dense(200, activation = get_activation_fn(activation_fn),\n",
    "                     kernel_initializer=get_initialization(activation_fn))(inputs)\n",
    "    \n",
    "    layer_e2 = Dense(200, activation = get_activation_fn(activation_fn),\n",
    "                     kernel_initializer=get_initialization(activation_fn))(layer_e1)\n",
    "    #Latent Space (no activation)\n",
    "    encoded = Dense(latent_layer_size)(layer_e2)\n",
    "       \n",
    "    layer_d1 = Dense(200, activation = get_activation_fn(activation_fn),\n",
    "                     kernel_initializer=get_initialization(activation_fn))(encoded)\n",
    "    \n",
    "    layer_d2 = Dense(200, activation = get_activation_fn(activation_fn),\n",
    "                     kernel_initializer=get_initialization(activation_fn))(layer_d1)\n",
    "    \n",
    "    decoded = Dense(window_length)(layer_d2)\n",
    "       \n",
    "    autoencoder = keras.models.Model(inputs=inputs, outputs = decoded)\n",
    "    \n",
    "    return autoencoder   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1127cac-d620-4cb3-a215-9ad3783dfcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 90)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               18200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 1005      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 200)               1200      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 90)                18090     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118,895\n",
      "Trainable params: 118,895\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Code Used\n"
     ]
    }
   ],
   "source": [
    "if AE_Model_Name == 'FC_Small':\n",
    "    AE_model = Small_AE(latent_layer_size = latent_layer_size)\n",
    "    AE_model.summary()\n",
    "    print('Code Used')\n",
    "else:\n",
    "    print('NOT Active')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b794a12-d1c9-4431-9617-9c657459ca34",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## FC_Medium\n",
    "-  (Input, 300, 300, 300, 300, Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8fca6-fa8a-46ad-9ab0-e6c8e1056c3d",
   "metadata": {},
   "source": [
    "### Sweep Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a55d409b-eedc-4fac-ab0f-49b728ee60b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT Active\n"
     ]
    }
   ],
   "source": [
    "if AE_Model_Name == 'FC_Medium':\n",
    "    \n",
    "    sweep_config = {\n",
    "    'method': 'random',\n",
    "    'name': Sweep_Config,\n",
    "    }\n",
    "    \n",
    "    metric = {\n",
    "    'name': 'mse',\n",
    "    'goal': 'minimize'\n",
    "    }\n",
    "    sweep_config['metric'] = metric\n",
    "\n",
    "    parameters_dict = {\n",
    "        'optimizer': {\n",
    "            'values': ['nadam', 'sgd']\n",
    "        },\n",
    "        'latent_layer_size': {\n",
    "            'value': latent_layer_size #previous [values: [5, 10, 25]\n",
    "        },\n",
    "        'epochs':{\n",
    "            'value': 100\n",
    "        },\n",
    "        'window_length':{\n",
    "            'value': int(dataset_name.split('_')[-1][1:])\n",
    "        },\n",
    "        'activation_fn':{\n",
    "            'values': ['SELU','LeakyReLU']\n",
    "        }\n",
    "    }\n",
    "    sweep_config['parameters'] = parameters_dict\n",
    "    \n",
    "    parameters_dict.update({\n",
    "        'learning_rate': {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 0.001,\n",
    "            'max': 0.1,\n",
    "          },\n",
    "        'batch_size': {\n",
    "            # integers between 2 and 256\n",
    "            # with evenly-distributed logarithms \n",
    "            'distribution': 'q_log_uniform_values',\n",
    "            'q': 2,\n",
    "            'min': 2,\n",
    "            'max': 256,\n",
    "          }\n",
    "        })\n",
    "    print('Code Used')\n",
    "else:\n",
    "    print('NOT Active')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4a35b-57b6-4790-b425-61225df17e55",
   "metadata": {},
   "source": [
    "### _Model_: FC_Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aa7b91c-5bfe-472b-b74e-80be8a00e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Medium_AE(window_length = 90, latent_layer_size = 5, activation_fn = 'SELU'):\n",
    "    \n",
    "    inputs = Input(shape= window_length)\n",
    "    \n",
    "    layer_e1 = Dense(300, activation = get_activation_fn(activation_fn),\n",
    "                     kernel_initializer=get_initialization(activation_fn))(inputs)    \n",
    "    layer_e2 = Dense(300, activation = get_activation_fn(activation_fn),\n",
    "                     kernel_initializer=get_initialization(activation_fn))(layer_e1)    \n",
    "    layer_e3 = Dense(300, activation = get_activation_fn(activation_fn),\n",
    "                     kernel_initializer=get_initialization(activation_fn))(layer_e2)    \n",
    "    layer_e4 = Dense(300, activation = get_activation_fn(activation_fn),\n",
    "                     kernel_initializer=get_initialization(activation_fn))(layer_e3)\n",
    "    #Latent Space (no activation)\n",
    "    encoded = Dense(latent_layer_size)(layer_e4)\n",
    "       \n",
    "    layer_d1 = Dense(300, activation = get_activation_fn(activation_fn),\n",
    "                     kernel_initializer=get_initialization(activation_fn))(encoded)\n",
    "    layer_d2 = Dense(300, activation = get_activation_fn(activation_fn),\n",
    "                     kernel_initializer=get_initialization(activation_fn))(layer_d1)\n",
    "    layer_d3 = Dense(300, activation = get_activation_fn(activation_fn),\n",
    "                     kernel_initializer=get_initialization(activation_fn))(layer_d2)\n",
    "    layer_d4 = Dense(300, activation = get_activation_fn(activation_fn),\n",
    "                     kernel_initializer=get_initialization(activation_fn))(layer_d3)    \n",
    "    decoded = Dense(window_length)(layer_d4)\n",
    "       \n",
    "    autoencoder = keras.models.Model(inputs=inputs, outputs = decoded)\n",
    "    \n",
    "    return autoencoder   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4adff6b2-6952-44ea-887f-7095a637e7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT Active\n"
     ]
    }
   ],
   "source": [
    "if AE_Model_Name == 'FC_Medium':\n",
    "    AE_model = Medium_AE(latent_layer_size = latent_layer_size)\n",
    "    AE_model.summary()\n",
    "    print('Code Used')\n",
    "else:\n",
    "    print('NOT Active')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e2102-26fb-4e24-978e-856983cbf259",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Run Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be1f0f15-f71d-4da8-8319-4104432b20f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sweep ID: ikwjt20r\n",
      "Sweep URL: https://wandb.ai/vasco-phd/FC_AE-Kenya_10k_Set_1_w90/sweeps/ikwjt20r\n"
     ]
    }
   ],
   "source": [
    "if use_running_sweep_id == True:\n",
    "    sweep_id = running_sweep_id\n",
    "    print(f'Using sweep ID: {sweep_id}')\n",
    "    print(f'Sweep URL: https://wandb.ai/vasco-phd/{Project_Name}/sweeps/{sweep_id}')\n",
    "else:\n",
    "    sweep_id = wandb.sweep(sweep_config, project = Project_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417746c-6cc3-4e07-9b8a-b8b9293d7017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 78yljuub with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: SELU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_layer_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0020135942433327637\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twindow_length: 90\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vasco-phd/FC_AE-Kenya_10k_Set_1_w90/runs/78yljuub\" target=\"_blank\">still-sweep-8</a></strong> to <a href=\"https://wandb.ai/vasco-phd/FC_AE-Kenya_10k_Set_1_w90\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/vasco-phd/FC_AE-Kenya_10k_Set_1_w90/sweeps/ikwjt20r\" target=\"_blank\">https://wandb.ai/vasco-phd/FC_AE-Kenya_10k_Set_1_w90/sweeps/ikwjt20r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17708/17710 [============================>.] - ETA: 0s - loss: 0.6355 - mse: 0.6355"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as activation_layer_call_fn, activation_layer_call_and_return_conditional_losses, activation_1_layer_call_fn, activation_1_layer_call_and_return_conditional_losses, activation_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17710/17710 [==============================] - 51s 3ms/step - loss: 0.6355 - mse: 0.6355 - val_loss: 0.6144 - val_mse: 0.6144 - lr: 0.0020\n",
      "Epoch 2/100\n",
      "17703/17710 [============================>.] - ETA: 0s - loss: 0.6037 - mse: 0.6037"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as activation_layer_call_fn, activation_layer_call_and_return_conditional_losses, activation_1_layer_call_fn, activation_1_layer_call_and_return_conditional_losses, activation_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17710/17710 [==============================] - 53s 3ms/step - loss: 0.6037 - mse: 0.6037 - val_loss: 0.6011 - val_mse: 0.6011 - lr: 0.0020\n",
      "Epoch 3/100\n",
      "17704/17710 [============================>.] - ETA: 0s - loss: 0.5921 - mse: 0.5921"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as activation_layer_call_fn, activation_layer_call_and_return_conditional_losses, activation_1_layer_call_fn, activation_1_layer_call_and_return_conditional_losses, activation_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17710/17710 [==============================] - 52s 3ms/step - loss: 0.5921 - mse: 0.5921 - val_loss: 0.5937 - val_mse: 0.5937 - lr: 0.0020\n",
      "Epoch 6/100\n",
      "17710/17710 [==============================] - 52s 3ms/step - loss: 0.5900 - mse: 0.5900 - val_loss: 0.5994 - val_mse: 0.5994 - lr: 0.0020\n",
      "Epoch 7/100\n",
      "17702/17710 [============================>.] - ETA: 0s - loss: 0.5883 - mse: 0.5883"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as activation_layer_call_fn, activation_layer_call_and_return_conditional_losses, activation_1_layer_call_fn, activation_1_layer_call_and_return_conditional_losses, activation_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17706/17710 [============================>.] - ETA: 0s - loss: 0.5857 - mse: 0.5857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as activation_layer_call_fn, activation_layer_call_and_return_conditional_losses, activation_1_layer_call_fn, activation_1_layer_call_and_return_conditional_losses, activation_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (C:\\Users\\ucesvpm\\OneDrive - University College London\\PhD Project\\Data Analytics\\Time Series Clustering\\Third Test\\wandb\\run-20230202_161559-78yljuub\\files\\model-best)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17710/17710 [==============================] - 53s 3ms/step - loss: 0.5857 - mse: 0.5857 - val_loss: 0.5870 - val_mse: 0.5870 - lr: 0.0020\n",
      "Epoch 11/100\n",
      "17710/17710 [==============================] - 46s 3ms/step - loss: 0.5627 - mse: 0.5627 - val_loss: 0.5691 - val_mse: 0.5691 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "17710/17710 [==============================] - 45s 3ms/step - loss: 0.5541 - mse: 0.5541 - val_loss: 0.5594 - val_mse: 0.5594 - lr: 5.0340e-04\n",
      "Epoch 26/100\n",
      "17710/17710 [==============================] - 50s 3ms/step - loss: 0.5520 - mse: 0.5520 - val_loss: 0.5679 - val_mse: 0.5679 - lr: 5.0340e-04\n",
      "Epoch 30/100\n",
      "17710/17710 [==============================] - 46s 3ms/step - loss: 0.5445 - mse: 0.5445 - val_loss: 0.5513 - val_mse: 0.5513 - lr: 2.5170e-04\n",
      "Epoch 41/100\n",
      "14735/17710 [=======================>......] - ETA: 6s - loss: 0.5441 - mse: 0.5441"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=sweep_train, count= sweep_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "432abfaa-0e16-401c-8c4b-26bed42d3854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ddc5f5-c6f0-4d3d-aecf-c2e4694bad51",
   "metadata": {},
   "source": [
    "---\n",
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e9638-c54d-4a4b-9d1d-ceec3bdb41cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
